---
title: "Categorical variables are more important than what you think"
description: |
  In many projects and datasets, we may face categorical features, among other feature types. This type of data shows qualitative information in datasets and has some finite categories. In this Dotto, we want to work with some different kinds of categorical features.
  
author:
  - name: "Mahyar Sharifi"
    occupation: "Internship"
    affiliation: "DataMotto"
    url: "https://www.linkedin.com/in/mahyar-sharifi"
    img: "Mahyar.jpg"
    lang: ["python", "r"]
tech:
  - lang: r
    pkgs: ["dyplyr", "palmerpenguins"]
  - lang: python
    pkgs: ["pandas", "seaborn"]
date:
  created: "2021-07-22"
  last_updated: "2021-07-22"
categories: "Wrangling"
applications: "General"
cover_image: "Mahyar.jpg"
slug: "descriptive-missing-values"
output: DataMotto::Dotto
---

```{r DataMotto, echo=FALSE}
DataMotto::use_Dotto()
```

<!-- Dot 1, lang: r,python ------------------------------------------------>

```{block, Dot = 1, part = "Instruction", lang = "r"}
The [Loan dataset](https://www.kaggle.com/burak3ergun/loan-data-set) is home loans information for Dream Housing Finance company. It contains 13 different features about their applicants. These features and their types are listed below:

- **Loan ID** : Applicants ID

- **Gender** : Categorical (*Female or Male*)

- **Married** : Categorical (*Yes or No*)

- **Dependents** : Categorical (*0, 1, 2, or 3+*)

- **Education** : Categorical (*Graduated or Not Graduated*)

- **Self-Employed** : Categorical (*Yes or No*)

- **Applicant Income** : Numerical 

- **Co-applicant Income** :  Numerical

- **Loan Amount** : Numerical

- **Loan-Amount-Term** : Categorical (*10 categories*)


- **Credit History** : Binary (*1 or 0*) 

- **Property Area** : Categorical(*Semiurban, Urban or Rural*)

- **Loan Status** : Categorical (*Y or N*)

In this Dotto, we want to count different categories in each categorical data, use suitable plots for visualization, encode and create dummy features.

Categorical data divides into two main classes:

- **Nominal** categorical data: In this type of data, we do not have any ordinal priority—for instance, weather conditions, medical conditions, and hair colors.

- **Ordinal** categorical data: In against an ordinal type, has ordinal priority among categories.

In the Loan dataset, Property Area and Education features are the examples of the nominal type, and Dependents is the example of the ordinal type.

```

```{block, Dot = 1, part = "Instruction", lang = "python"}
The [Loan dataset](https://www.kaggle.com/burak3ergun/loan-data-set) is home loans information for Dream Housing Finance company. It contains 13 different features about their applicants. These features and their types are listed below:

- **Loan ID** : Applicants ID

- **Gender** : Categorical (*Female or Male*)

- **Married** : Categorical (*Yes or No*)

- **Dependents** : Categorical (*0, 1, 2, or 3+*)

- **Education** : Categorical (*Graduated or Not Graduated*)

- **Self-Employed** : Categorical (*Yes or No*)

- **Applicant Income** : Numerical 

- **Co-applicant Income** :  Numerical

- **Loan Amount** : Numerical

- **Loan-Amount-Term** : Categorical (*10 categories*)

- **Credit History** : Binary (*1 or 0*) 

- **Property Area** : Categorical(*Semiurban, Urban or Rural*)

- **Loan Status** : Categorical (*Y or N*)

In this Dotto, we want to count different categories in each categorical data, use suitable plots for visualization, encode and create dummy features.

Categorical data divides into two main classes:

- **Nominal** categorical data: In this type of data, we do not have any ordinal priority—for instance, weather conditions, medical conditions, and hair colors.

- **Ordinal** categorical data: In against an ordinal type, has ordinal priority among categories.

In the Loan dataset, Property Area and Education features are the examples of the nominal type, and Dependents is the example of the ordinal type.

```

```{r, Dot = 1, part = "Code", echo = T, eval = F}
#Import Packages
library(dplyr)
library(ggplot2)

#Load Dataset
data = read.csv('loan_data_set.csv',na.strings=c(""))

#Drop NA values
data = na.omit(data)

#Show five sample of dataset
sample_n(data, 5)

```

```{python, Dot = 1, part = "Code", echo = T, eval = F}
#Import packages
import pandas as pd
import seaborn as sns
import numpy as np

#Load dataset
data = pd.read_csv('loan_data_set.csv')

#Show dataset information
data.info()

#Drop Na
data = data.dropna()

#show dataset samples
data.sample(5)

```

```{r, Dot = 1, part = "Result", echo = F, eval = T}
options(warn=-1)

#Import Packages
library(dplyr)
library(ggplot2)

#Load Dataset
data = read.csv('loan_data_set.csv',na.strings=c(""))

#Drop NA values
data = na.omit(data)

#Show five sample of dataset
sample_n(data, 5)

```

```{python, Dot = 1, part = "Result", echo = F, eval = T}
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

#Import packages
import pandas as pd
import seaborn as sns
import numpy as np

#Load dataset
data = pd.read_csv('loan_data_set.csv')

#Show dataset information
data.info()

#Drop Na
data = data.dropna()

#show dataset samples
data.sample(5)

```

<!-- Dot 2, lang: r,python ------------------------------------------------>

```{block, Dot = 2, part = "Instruction", lang = "r"}
It is essential to know the frequency of each category in categorical features. For this purpose, we can use table to create a table or use `ggplot` to plot a barplot. For example, in the Loan dataset, we have some categorical features like Property Area. This feature has three categories, and we want to know the most frequent category. We can use both table and plot to identify it. 
In this dot, first we use `fct_reorder` in `forcats` package to sort categories by their frequency then use `ggplot` to plot a barplot.

```

```{block, Dot = 2, part = "Instruction", lang = "python"}
It is essential to know the frequency of each category in categorical features. For this purpose, we can use `value_counts` to create a table or use `catplot` to plot a barplot. For example, in the Loan dataset, we have some categorical features like Property Area. This feature has three categories, and we want to know the most frequent category. We can use both table and plot to identify it. 

```

```{r, Dot = 2, part = "Code", echo = T, eval = F}
#Count frequency of 'Property_Area' categories
table(data$Property_Area)

#Import packages
library(ggplot2)
library(forcats)

#Plot bar plot for 'Property_Area'  feature
data %>%
    count(Property_Area) %>%
    mutate(Property_Area = 
             fct_reorder(Property_Area, n, .desc = TRUE)) %>%
    ggplot(aes(x = Property_Area,
               y = n,
               fill=Property_Area))+
    geom_bar(stat = 'identity')   

```

```{python, Dot = 2, part = "Code", echo = T, eval = F}
#Count frequency of 'Property_Area' categories
print(data['Property_Area'].value_counts())

#Plot bar plot for 'Property_Area'  feature
sns.catplot(data = data,
            x = 'Property_Area',
            kind = 'count')

```

```{r, Dot = 2, part = "Result", echo = F, eval = T}
#Count frequency of 'Property_Area' categories
table(data$Property_Area)

#Import packages
library(ggplot2)
library(forcats)

#Plot bar plot for 'Property_Area'  feature
data %>%
    count(Property_Area) %>%
    mutate(Property_Area = 
             fct_reorder(Property_Area, n, .desc = TRUE)) %>%
    ggplot(aes(x = Property_Area,
               y = n,
               fill=Property_Area))+
    geom_bar(stat = 'identity')  

```

```{python, Dot = 2, part = "Result", echo = F, eval = T}
#Count frequency of 'Property_Area' categories
print(data['Property_Area'].value_counts())

#Plot bar plot for 'Property_Area'  feature
sns.catplot(data = data, x = 'Property_Area', kind = 'count')

```

<!-- Dot 3, lang: r,python ------------------------------------------------>

```{block, Dot = 3, part = "Instruction", lang = "r"}
Sometimes it is good to know the variation in numerical features in each category in a categorical feature. For this purpose, we can plot a `stripplot`. For example, we want to show the Loan Amount variation in each Dependents category in the Loan dataset. We can plot a strip plot by `ggplot` and `geom_jitter` in the `ggplot2` package.

```

```{block, Dot = 3, part = "Instruction", lang = "python"}
Sometimes it is good to know the variation in numerical features in each category in a categorical feature. For this purpose, we can plot a `stripplot`. For example, we want to show the Loan Amount variation in each Dependents category in the Loan dataset. We can plot a strip plot by `catplot` in the `seaborn` package and set the kind with the `strip`.

```

```{r, Dot = 3, part = "Code", echo = T, eval = F}
#PLot a stripplot for each 'Dependets' categories for 'LoanAmount'
ggplot(data, aes(x = Dependents, y = LoanAmount, color = Dependents))+
geom_jitter()+
theme(legend.position = "none")

```

```{python, Dot = 3, part = "Code", echo = T, eval = F}
#PLot a stripplot for each 'Dependets' category for 'LoanAmount'
sns.catplot(data = data,
            x = 'Dependents',
            y = 'LoanAmount')

```

```{r, Dot = 3, part = "Result", echo = F, eval = T}
#PLot a stripplot for each 'Dependets' categories for 'LoanAmount'
ggplot(data, aes(x = Dependents, y = LoanAmount, color = Dependents))+
geom_jitter()+
theme(legend.position = "none")

```

```{python, Dot = 3, part = "Result", echo = F, eval = T}
#PLot a stripplot for each 'Dependets' category for 'LoanAmount'
sns.catplot(data = data, x = 'Dependents', y = 'LoanAmount')

```

<!-- Dot 4, lang: r,python ------------------------------------------------>

```{block, Dot = 4, part = "Instruction", lang = "r"}
In the last dot, we may see some outliers in some categories. One of the good ways to identify outliers is to plot the box plot. For this purpose, we can use `geom_boxplot` to create a boxplot for each category. 
As we can see in the box plot, the zero dependents category has many outliers. 

```

```{block, Dot = 4, part = "Instruction", lang = "python"}
In the last dot, we may see some outliers in some categories. One of the good ways to identify outliers is to plot the box plot. For this purpose, we can use `catplot` and set the kind with the `box` to create a boxplot for each category. 
As we can see in the box plot, the zero dependents category has many outliers. 

```

```{r, Dot = 4, part = "Code", echo = T, eval = F}
#PLot a boxplot for each 'Dependets' categories for 'LoanAmount'
ggplot(data, aes(x=Dependents,
                 y=LoanAmount,
                 color = Dependents)) +
  geom_boxplot() +
  theme(legend.position = "none")

```

```{python, Dot = 4, part = "Code", echo = T, eval = F}
#PLot a boxplot for each 'Dependets' categories for 'LoanAmount'
sns.catplot(data = data,
            x = 'Dependents',
            y = 'LoanAmount',
            kind = 'box')

```

```{r, Dot = 4, part = "Result", echo = F, eval = T}
#PLot a boxplot for each 'Dependets' categories for 'LoanAmount'
ggplot(data, aes(x=Dependents,
                 y=LoanAmount,
                 color = Dependents)) +
  geom_boxplot() +
  theme(legend.position = "none")

```

```{python, Dot = 4, part = "Result", echo = F, eval = T}
#PLot a boxplot for each 'Dependets' categories for 'LoanAmount'
sns.catplot(data = data, x = 'Dependents', y = 'LoanAmount', kind = 'box')

```

<!-- Dot 5, lang: r,python ------------------------------------------------>

```{block, Dot = 5, part = "Instruction", lang = "r"}
Feature engineering is one of the most crucial processes to prepare categorical data for machine learning projects. In feature engineering, we use some transformation techniques to transform categorical data into some numerical features. For this purpose, we can use the `LabelEncoder` in `superml` package to encode features. In this dot, we use `LabelEncoder` to transform Property Area categories into numeric values.

```

```{block, Dot = 5, part = "Instruction", lang = "python"}
Feature engineering is one of the most crucial processes to prepare categorical data for machine learning projects. In feature engineering, we use some transformation techniques to transform categorical data into some numerical features. For this purpose, we can use the `LabelEncoder()` in `sklearn` package to encode features. In this dot, we use `LabelEncoder` to transform Property Area categories into numeric values.

```

```{r, Dot = 5, part = "Code", echo = T, eval = F}
#Import superml package
library(superml)

#Select 'property_Area'
df_Property_Area <- data %>% select(Property_Area)

#Fit and Transform LabelEncoder 
Property_Area_encoded <- LabelEncoder$new()

print(Property_Area_encoded$fit(df_Property_Area$Property_Area))

df_Property_Area$Property_Area_encoded <-
  Property_Area_encoded$fit_transform(
    df_Property_Area$Property_Area)

```

```{python, Dot = 5, part = "Code", echo = T, eval = F}
#Import LabelEncoder from sklearn
from sklearn.preprocessing import LabelEncoder

#Fit and Transform LabelEncoder 
le = LabelEncoder()
Property_labels = le.fit_transform(
                        data[['Property_Area']])

#Show encoded categories
genre_mappings = {index: label for index,
                  label in enumerate(le.classes_)}
print(genre_mappings)

```

```{r, Dot = 5, part = "Result", echo = F, eval = T}
#Import superml package
library(superml)

#Select 'property_Area'
df_Property_Area <- data %>% select(Property_Area)

#Fit and Transform LabelEncoder 
Property_Area_encoded <- LabelEncoder$new()

print(Property_Area_encoded$fit(df_Property_Area$Property_Area))

df_Property_Area$Property_Area_encoded <-
  Property_Area_encoded$fit_transform(
    df_Property_Area$Property_Area)

```

```{python, Dot = 5, part = "Result", echo = F, eval = T}
#Import LabelEncoder from sklearn
from sklearn.preprocessing import LabelEncoder

#Fit and Transform LabelEncoder 
le = LabelEncoder()
Property_labels = le.fit_transform(
                        data[['Property_Area']])

#Show encoded categories
genre_mappings = {index: label for index, label in enumerate(le.classes_)}
genre_mappings

```

<!-- Dot 6, lang: r,python ------------------------------------------------>

```{block, Dot = 6, part = "Instruction", lang = "r"}
Use the One-Hot (or dummy) encoding scheme is another way for feature engineering for categorical data. In this scheme, each category transforms into one binary feature. For example, if a categorical feature has *m* categories, then after One-Hot transformation, we have *m* binary feature, and every instance has only one of the values as 1. In python, we can use the `dummy_cols` in`fastDummies` to use One-Hot transformation. In this dot, we transform **Property Area** categories into three binary features.

```

```{block, Dot = 6, part = "Instruction", lang = "python"}
Use the One-Hot encoding scheme is another way for feature engineering for categorical data. In this scheme, each category transforms into one binary feature. For example, if a categorical feature has *m* categories, then after One-Hot transformation, we have *m* binary feature, and every instance has only one of the values as 1. In python, we can use the `OneHotEncoder()` in`sklearn` to use One-Hot transformation. In this dot, we transform **Property Area** categories into three binary features.

```

```{r, Dot = 6, part = "Code", echo = T, eval = F}
#Import fastDummies package
library(fastDummies)

#Dummy 'Propert_Area' categories
sample_n(dummy_cols(df_Property_Area,
                    select_columns = c("Property_Area")),5)

```

```{python, Dot = 6, part = "Code", echo = T, eval = F}
#Import LabelEncoder from sklearn
from sklearn.preprocessing import OneHotEncoder

#Fit and Transform OneHotEncoder 
ohe = OneHotEncoder()
Property_one_hot = ohe.fit_transform(
                            data[['Property_Area']])

#Show encoded categories as pandas dataframe
ohe_features = pd.DataFrame(Property_one_hot.toarray(), 
                            columns=
                            ['Rural', 'Semiurban', 'Urban'])
ohe_features.head(5)

```

```{r, Dot = 6, part = "Result", echo = F, eval = T}
#Import fastDummies package
library(fastDummies)

#Dummy 'Propert_Area' categories
sample_n(dummy_cols(df_Property_Area,
                    select_columns = c("Property_Area")),5)

```

```{python, Dot = 6, part = "Result", echo = F, eval = T}
#Import LabelEncoder from sklearn
from sklearn.preprocessing import OneHotEncoder

#Fit and Transform OneHotEncoder 
ohe = OneHotEncoder()
Property_one_hot = ohe.fit_transform(data[['Property_Area']])

#Show encoded categories as pandas dataframe
ohe_features = pd.DataFrame(Property_one_hot.toarray(), 
                            columns=['Rural', 'Semiurban', 'Urban'])
ohe_features.head(5)

```

<!-- Dot 7, lang: r,python ------------------------------------------------>

```{block, Dot = 7, part = "Instruction", lang = "r"}
Sometimes we want to change the category name in order manually. For example, the Dependents feature is an ordinal category in the Loan dataset, and we want to change just '3+' into '3'. In this situation, with the `recode_factor` function, we can define any name for categories.

```

```{block, Dot = 7, part = "Instruction", lang = "python"}
Sometimes we want to change the category name in order manually. For example, the Dependents feature is an ordinal category in the Loan dataset, and we want to change just '3+' into '3'. In this situation, with the `map` function, we can define any name for categories.

```

```{r, Dot = 7, part = "Code", echo = T, eval = F}
#Select 'Dependents' feature
df_dependents <- data %>% 
          select(Dependents)

#Recode the levels
df_dependents$Dependents <- recode_factor(df_dependents$Dependents,
                                          "3+" = "3" )

```

```{python, Dot = 7, part = "Code", echo = T, eval = F}
#Create mapping dictionary for 'Dependents' categories
dependents_map = { '0' : 0, '1' : 1, '2' : 2, '3+' : 3}
data['Dependents'] = data['Dependents'].
                      map(dependents_map)

#Print new categories
print(data['Dependents'].unique())

```

```{r, Dot = 7, part = "Result", echo = F, eval = T}
#Select 'Dependents' feature
df_dependents <- data %>% 
          select(Dependents)

#Recode the levels
df_dependents$Dependents <- recode_factor(df_dependents$Dependents,
                                          "3+" = "3" )

```

```{python, Dot = 7, part = "Result", echo = F, eval = T}
#Create mapping dictionary for 'Dependents' categories
dependents_map = { '0' : 0, '1' : 1, '2' : 2, '3+' : 3}
data['Dependents'] = data['Dependents'].map(dependents_map)

#Print new categories
print(data['Dependents'].unique())

```

<!-- Dot 8, lang: r,python ------------------------------------------------>

```{block, Dot = 8, part = "Instruction", lang = "r"}
We have features like **Gender**, **Married**, **Education**, **Self-Employed**, and **Loan-Status** in the Loan dataset with only two categories. In this situation, it is better to transform these features into binary features. For this purpose, we can use `dummy_cols` in fastdummies package then remove first column or encode manually with the recode_factor function.

```

```{block, Dot = 8, part = "Instruction", lang = "python"}
We have features like **Gender**, **Married**, **Education**, **Self-Employed**, and **Loan-Status** in the Loan dataset with only two categories. In this situation, it is better to transform these features into binary features. For this purpose, we can use `LabelEncoder` in sklearn package or encode manually with the map function.

```

```{r, Dot = 8, part = "Code", echo = T, eval = F}
#Select features
df_bin <- data %>% 
          select(Gender,Married,
                 Self_Employed, Loan_Status)

#Dummy features and drop first column
dummy_cols(df_bin,
                select_columns = c("Gender" ),
                remove_first_dummy = TRUE)

```

```{python, Dot = 8, part = "Code", echo = T, eval = F}
#Print Unique values
print(pd.unique(data['Gender']))
print(pd.unique(data['Married']))
print(pd.unique(data['Self_Employed']))
print(pd.unique(data['Loan_Status']))

#Create mapping categories
gender_level = {'Male':1, 'Female':0}
married_level = {'Yes':1, 'No':0}
self_employed_level = {'Yes':1, 'No':0}
Loan_status = {'Y':1, 'N':0}

#Mapping with new categories
data['Gender'] = data['Gender'].map(gender_level)
data['Married'] = data['Married'].map(married_level)
data['Self_Employed'] = data['Self_Employed'].map(self_employed_level)
data['Loan_Status'] = data['Loan_Status'].map(Loan_status)

```

```{r, Dot = 8, part = "Result", echo = F, eval = T}
#Select features
df_bin <- data %>% 
          select(Gender,Married,
                 Self_Employed, Loan_Status)

#Dummy features and drop first column
dummy_cols(df_bin,
                select_columns = c("Gender" ),
                remove_first_dummy = TRUE)

```

```{python, Dot = 8, part = "Result", echo = F, eval = T}
#Print Unique values
print(pd.unique(data['Gender']))
print(pd.unique(data['Married']))
print(pd.unique(data['Self_Employed']))
print(pd.unique(data['Loan_Status']))

#Create mapping categories
gender_level = {'Male':1, 'Female':0}
married_level = {'Yes':1, 'No':0}
self_employed_level = {'Yes':1, 'No':0}
Loan_status = {'Y':1, 'N':0}

#Mapping with new categories
data['Gender'] = data['Gender'].map(gender_level)
data['Married'] = data['Married'].map(married_level)
data['Self_Employed'] = data['Self_Employed'].map(self_employed_level)
data['Loan_Status'] = data['Loan_Status'].map(Loan_status)

```







